---
title: "A log-ratio approach to cluster analysis of count data when the total is irrelevant"
subtitle: ""
author: "Marc Comas-Cufí"
institute: "Department of Computer Science, Applied Mathematics and Statistics"
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r, include=FALSE}
set.seed(1)
library(knitr)
library(data.table)
library(flextable)
library(ggplot2)
library(ggtern)
library(coda.base)
library(coda.count)
library(mclust)
library(magrittr)
library(fpc)
library(diceR)
source('../plotting.R')
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE, comment = '#> ', warning=FALSE, cache = TRUE)
theme_set(theme_minimal() + theme(legend.position = 'none'))
# print.matrix = function(x, ...){
#   print(class(x))
#   if(is.matrix(x)){
#     print.default(round(x, 3), ...)
#   }else{
#     print.default(x, ...)
#   }
# }
```

class: top

# Multivariate count data: **Multinomial** vs **Multivariate Poisson**

* Without variability in their parameters (with constant parameters).
    * Parts of __multivariate Poisson__ observations are independent. 
    * Parts of __multinomial__ observations are negatively correlated $\implies$ Tendency of negative bias $\implies$ interpretation about association of parts can not depend on $cov(X_i,X_j)$.

--

```{r}
N = 1000000; LAMBDA = c(2,2,4); PI = c(0.25,0.25,0.5)

cor(sapply(LAMBDA, rpois, n = N))
cor(coda.count::rmultinomial(n = N, size = 8, p = PI))
```

---

#### Multivariate count data: **Multinomial** vs **Multivariate Poisson**

* Without variability in their parameters (with constant parameters).
    * Parts of __multivariate Poisson__ observations are independent. 
    * Parts of __multinomial__ observations are negatively correlated


In other words,

* $\text{cov}(X_i,X_j) = 0$ for the __multivariate Poisson__.
* $\text{cov}(X_i,X_j) < 0$ $\implies$ Tendency of negative bias $\implies$ interpretation about association of parts can not depend on $\text{cov}(X_i,X_j)$.

---

#### Relation to strictly positive samples

* __Multivariate Poisson__ distributed data $\iff$ __Log-normally__  distributed data
* __Multinomial__ distributed data $\iff$ __Log-ratio-normally__  distributed data

```{r}
N = 1000000; MU = c(2,2,3)
lnX = exp(sapply(MU, function(m) rnorm(n=N,m)))

cor(lnX)
cor(lnX/rowSums(lnX))
```

---


#### Parametric approaches to cluster multivariate count data (1)

* __Ignoring compositional variability and ignoring counting variabily__. 
    * Consider data is real, i.e. $X \in \mathbb{R}^d$ and apply classical methods.

--

* __Taking into account counting variability, but ignoring compositional variabity__. Mixtures of multinomials distributions.

--

* __Taking into account compositional variability, but ignoring counting variability__. Zero replacement methods followed by classical compositional methods.
    * Dirichlet Prior (Martin-Fernandez _et al._, 2015)
    * Log-ratio Prior (Comas-Cufí _et al._, 2019)
<!--    * We have $a=b$ when $a = (1,1,2)$, $b=(1000,1000,2000)$. $a$ and $b$ at the same distance than $p=(\frac{1}{3},\frac{1}{3},\frac{1}{3})$. -->

---

#### Parametric approaches to cluster multivariate count data (2)

* __Taking into account compositional and counting nature__.

    * _Topic models_. Mixture of multinomials where mixing proportions are modelled in the Simplex.
    
        * __Latent Dirichlet Allocation__.  Dirichlet distribution. (Blei _et. al., 2003)
        * __Correlated Topic Models__.  Log-ratio normal distribution. (Blei & Lafferty, 2007)
        
    * _Mixture of compounding distributions_.
    
        * __Mixtures of Dirichlet-multinomial distributions__. (Holmes _et al._, 2012)
        * __Mixtures of log-ratio-normal-multinomial distributions__. (Comas-Cufí _et al._, 2017)

---

#### Using classical clustering approach for real data to cluster count data

1. __Dealing with zeros__. Fit a Dirichlet-multinomial distribution to your data.
    * _A Dirichlet prior seems to be conservative in keeping the covariance structure observed in count data_.  Regression toward the mean is moderate.
1. __Compositional variability__. Fit a mixture of log-ratio spherical Gaussian distributions to the posterior probabilities.
1. __Counting variability__. Create $B$ new samples using the posterior distribution and find a cluster using a classical method.
1. __Combining results__. Use cumulative voting (Dudoit & Fridlyand, 2003) to build a cluster.

---

### Simple example based on 2017 catalan parliament elections

```{r}
load('parliament2017.RData')
parliament
```


---

### We only consider three parts

```{r}
Y = matrix(0, nrow = nrow(parliament), ncol = 3)
Y[,1] = with(parliament, catsp+other)
Y[,2] = with(parliament, jxcat+erc+cup) 
Y[,3] = with(parliament, cs+psc+pp)
colnames(Y) = c('other', 'ind', 'esp')
```

```{r, echo=FALSE, out.width="50%", fig.align='center'}
n0 = apply(Y,1,min)== 0
n1 = apply(Y,1,min)== 1
n2 = apply(Y,1,min)== 2
ggtern() +
  geom_mask() +
  geom_point(data = data.table(Y), aes(x=other, y=ind, z=esp), alpha=0.8) +
  geom_point(data = data.table(Y)[n0], aes(x=other, y=ind, z=esp), col = 'red', alpha=0.8) +
  # geom_point(data = data.table(Y)[n1], aes(x=other, y=ind, z=esp), col = 'orange', alpha=0.8) +
  # geom_point(data = data.table(Y)[n2], aes(x=other, y=ind, z=esp), col = 'yellow', alpha=0.8) +
  labs(title = '')
```

---

### Dealing with zeros

```{r, include=FALSE}
ZR_LAB = 'Zero replacement'
NZ_LAB = 'Dirichlet-multinomial smoothing'
Yzr = data.table(zCompositions::cmultRepl(Y, suppress.print = T))
fit = fit_dm(Y)
Ynz = t(t(Y) + fit[,1])
Ynz = data.table(Ynz/rowSums(Ynz)*rowSums(Y))
```

```{r, echo=FALSE, out.width="48%", fig.show='hold'}
ggtern() +
  geom_mask() +
  geom_point(data = Yzr, aes(x=other, y=ind, z=esp), alpha=0.8) +
  geom_point(data = Yzr[n0], aes(x=other, y=ind, z=esp), col = 'red', alpha=0.8) +
  geom_point(data = Yzr[n1], aes(x=other, y=ind, z=esp), col = 'orange', alpha=0.8) +
  geom_point(data = Yzr[n2], aes(x=other, y=ind, z=esp), col = 'yellow', alpha=0.8) +
  labs(title = ZR_LAB)
ggtern() +
  geom_mask() +
  geom_point(data = Ynz, aes(x=other, y=ind, z=esp), alpha=0.8) +
  geom_point(data = Ynz[n0], aes(x=other, y=ind, z=esp), col = 'red', alpha=0.8) +
  geom_point(data = Ynz[n1], aes(x=other, y=ind, z=esp), col = 'orange', alpha=0.8) +
  geom_point(data = Ynz[n2], aes(x=other, y=ind, z=esp), col = 'yellow', alpha=0.8) +
  labs(title = NZ_LAB)
```

---

### Basis selection

```{r, include=FALSE}
B = ilr_basis(3)[c(3,1,2),]
rownames(B) = colnames(Ynz)
colnames(B) = paste0('B', 1:2)
B2 = B[,1:2]
```

```{r, echo=FALSE, out.width="50%", fig.show='hold'}
XLIM = c(-2, 4)
YLIM = c(0, 4.5)
Hzr = data.table(coordinates(Yzr, B2, label = 'ilr'))
p0_zr = ggplot() +
  geom_point(data = Hzr, aes(ilr1, ilr2)) +
  geom_point(data = Hzr[n0], aes(ilr1, ilr2), col = 'red') +
  # geom_point(data = Hzr[n1], aes(ilr1, ilr2), col = 'orange') +
  # geom_point(data = Hzr[n2], aes(ilr1, ilr2), col = 'yellow') +
  labs(title = 'Coordinates', subtitle = ZR_LAB) +
  coord_fixed(xlim = XLIM, ylim = YLIM)
p0_zr
#####
Hnz = coordinates(Ynz, B2, label = 'ilr')
p0_nz = ggplot() +
  geom_point(data = Hnz, aes(ilr1, ilr2)) +
  geom_point(data = Hnz[n0], aes(ilr1, ilr2), col = 'red') +
  # geom_point(data = Hnz[n1], aes(ilr1, ilr2), col = 'orange') +
  # geom_point(data = Hnz[n2], aes(ilr1, ilr2), col = 'yellow') +
  labs(title = 'Coordinates', subtitle = NZ_LAB) +
  coord_fixed(xlim = XLIM, ylim = YLIM)
p0_nz
```

```{r basis, echo=FALSE}
BASIS = as.data.table(t(sign(B2)),keep.rownames=TRUE)
names(BASIS)[1]=  'Basis:'
BASIS %>%
  flextable() %>%
  colformat_int(col_keys = colnames(Y))
```

---

### Modeling compositional variability

```{r, include=FALSE}
Mzr = Mclust(Hzr, G = 10, modelNames = "EVV")
Mnz = Mclust(Hnz, G = 10, modelNames = "EVV")
Lzr = data_mixture(xseq = seq(-2,4, length.out = 100), yseq = seq(0, 4.5, length.out = 100), Mzr) %>%
  lapply(as.data.table)
Lnz = data_mixture(xseq = seq(-2,4, length.out = 100), yseq = seq(0, 4.5, length.out = 100), Mnz) %>%
  lapply(as.data.table)
```

```{r, echo=FALSE, out.width="50%", fig.show='hold'}
p1_zr = ggplot() +
  geom_point(data = Hzr, aes(ilr1, ilr2), alpha=0.2)
for(l in Lzr){
  p1_zr = p1_zr +
    geom_path(data=l, aes(x,y))
}
p1_zr = p1_zr +
  coord_fixed(xlim = XLIM, ylim = YLIM)
p1_zr + labs(title = 'Coordinates', subtitle = ZR_LAB)
#############
p1_nz = ggplot() +
  geom_point(data = Hnz, aes(ilr1, ilr2), alpha=0.2)
for(l in Lnz){
  p1_nz = p1_nz +
    geom_path(data=l, aes(x,y))
}
p1_nz = p1_nz +
  coord_fixed(xlim = XLIM, ylim = YLIM)
p1_nz + labs(title = 'Coordinates', subtitle = NZ_LAB)
```

```{r, ref.label='basis', echo=FALSE}
```

---

### Modeling count variability (1)

```{r, include=FALSE}
I = 60 #66 #413#1#60
```

```{r posterior_contour, include=FALSE}
set.seed(1)
#########
PI_zr = Mzr$parameters$pro
MU_zr = Mzr$parameters$mean
SIGMA_zr = Mzr$parameters$variance$sigma
H_zr_s = c_rlrnm_mixture_posterior(500, Y[I,], PI_zr, MU_zr, SIGMA_zr, B2, 1000) %>% as.data.frame()
l_post_zr = data_posterior(xseq = seq(min(H_zr_s[,1]), max(H_zr_s[,1]), length.out = 100),
                        yseq = seq(min(H_zr_s[,2]), max(H_zr_s[,2]), length.out = 100),
                        Y[I,], PI_zr, MU_zr, SIGMA_zr, B2) %>%
  lapply(as.data.table)
########
PI_nz = Mnz$parameters$pro
MU_nz = Mnz$parameters$mean
SIGMA_nz = Mnz$parameters$variance$sigma
H_nz_s = c_rlrnm_mixture_posterior(500, Y[I,], PI_nz, MU_nz, SIGMA_nz, B2, 1000) %>% as.data.frame()
l_post_nz = data_posterior(xseq = seq(min(H_nz_s[,1]), max(H_nz_s[,1]), length.out = 100),
                        yseq = seq(min(H_nz_s[,2]), max(H_nz_s[,2]), length.out = 100),
                        Y[I,], PI_nz, MU_nz, SIGMA_nz, B2) %>%
  lapply(as.data.table)
```

```{r plot_posterior,  echo=FALSE, out.width="50%", fig.show='hold'}
p2_zr = p1_zr
for(post in l_post_zr){
  p2_zr = p2_zr + geom_path(data = post,  aes(x = x, y = y), col = 'blue', alpha=0.4)
}
p2_zr = p2_zr +
  geom_point(data = Hzr[I,], aes(x = ilr1, y = ilr2), col = 'blue', size = 2)
# p2_zr = p2_zr +
#   geom_point(data=H_zr_s, aes(x=V1,y=V2))
p2_zr + labs(title = 'Coordinates', subtitle = ZR_LAB)

p2_nz = p1_nz
for(post in l_post_nz){
  p2_nz = p2_nz + geom_path(data = post,  aes(x = x, y = y), col = 'blue', alpha=0.4)
}
p2_nz = p2_nz +
  geom_point(data = Hnz[I,], aes(x = ilr1, y = ilr2), col = 'blue', size = 2)
p2_nz + labs(title = 'Coordinates', subtitle = NZ_LAB)
```

```{r municipality, echo=FALSE}
flextable(parliament[I,])
```

---

### Modeling count variability (2)

```{r, include=FALSE}
I = 66 #66 #413#1#60
```

```{r, ref.label='posterior_contour', include=FALSE}
```

```{r, ref.label='plot_posterior', echo=FALSE, out.width="50%", fig.show='hold'}
```

```{r, ref.label='municipality', echo=FALSE}
```

---

### Modeling count variability (3)

```{r, include=FALSE}
I = 90 #66 #413#1#60
```

```{r, ref.label='posterior_contour', include=FALSE}
```

```{r, ref.label='plot_posterior', echo=FALSE, out.width="50%", fig.show='hold'}
```

```{r, ref.label='municipality', echo=FALSE}
```

---

### Taking into account compositional and count variability

```{r plot_sampling0,  echo=FALSE, out.width="50%", fig.show='hold'}
Hs_zr = apply(Y, 1, function(x_) c_rlrnm_mixture_posterior(1, x_, PI_zr, MU_zr, SIGMA_zr, t(MASS::ginv(B2)), r = 100)) %>% 
  t() %>% as.data.table()
Hs_nz = apply(Y, 1, function(x_) c_rlrnm_mixture_posterior(1, x_, PI_nz, MU_nz, SIGMA_nz, t(MASS::ginv(B2)), r = 100)) %>% 
  t() %>% as.data.table()

p3_zr = ggplot() +
  geom_point(data = Hs_zr, aes(V1, V2), alpha=0.2)
for(l in Lzr){
  p3_zr = p3_zr +
    geom_path(data=l, aes(x,y))
}
p3_zr = p3_zr +
  coord_fixed(xlim = XLIM, ylim = YLIM)
p3_zr + labs(title = 'Coordinates', subtitle = ZR_LAB)

p3_nz = ggplot() +
  geom_point(data = Hs_nz, aes(V1, V2), alpha=0.2)
for(l in Lnz){
  p3_nz = p3_nz +
    geom_path(data=l, aes(x,y))
}
p3_nz = p3_nz +
  coord_fixed(xlim = XLIM, ylim = YLIM)
p3_nz + labs(title = 'Coordinates', subtitle = NZ_LAB)
```

---

### Taking into account compositional and count variability

```{r, echo=FALSE}
gen_sample_zr = function(){
  Hs_zr = apply(Y, 1, function(x_) c_rlrnm_mixture_posterior(1, x_, PI_zr, MU_zr, SIGMA_zr, t(MASS::ginv(B2)), r = 100)) %>% 
  t() %>% as.data.table()
  Hs_zr[,cl := paste0('cl',kmeansruns(.SD, krange = 1:10)$cluster)]
  Hs_zr
}
gen_sample_nz = function(){
  Hs_nz = apply(Y, 1, function(x_) c_rlrnm_mixture_posterior(1, x_, PI_nz, MU_nz, SIGMA_nz, t(MASS::ginv(B2)), r = 100)) %>% 
  t() %>% as.data.table()
  Hs_nz[,cl := paste0('cl',kmeansruns(.SD, krange = 1:10)$cluster)]
  Hs_nz
}
```

```{r plot_sampling,  echo=FALSE, out.width="50%", fig.show='hold'}
Hs_zr = gen_sample_zr()
Hs_nz = gen_sample_nz()
p3_zr = ggplot() +
  geom_point(data = Hs_zr, aes(V1, V2, col=cl))
for(l in Lzr){
  p3_zr = p3_zr +
    geom_path(data=l, aes(x,y))
}
p3_zr = p3_zr +
  coord_fixed(xlim = XLIM, ylim = YLIM)
p3_zr + labs(title = 'Coordinates', subtitle = ZR_LAB)

p3_nz = ggplot() +
  geom_point(data = Hs_nz, aes(V1, V2, col=cl))
for(l in Lnz){
  p3_nz = p3_nz +
    geom_path(data=l, aes(x,y))
}
p3_nz = p3_nz +
  coord_fixed(xlim = XLIM, ylim = YLIM)
p3_nz + labs(title = 'Coordinates', subtitle = NZ_LAB)
```

---

### Taking into account compositional and count variability

```{r, ref.label='plot_sampling',  echo=FALSE, out.width="50%", fig.show='hold'}
```

---

### Taking into account compositional and count variability

```{r, ref.label='plot_sampling',  echo=FALSE, out.width="50%", fig.show='hold'}
```

---

### Taking into account compositional and count variability

```{r, ref.label='plot_sampling',  echo=FALSE, out.width="50%", fig.show='hold'}
```

---

### Taking into account compositional and count variability

```{r, ref.label='plot_sampling',  echo=FALSE, out.width="50%", fig.show='hold'}
```

---

### Taking into account compositional and count variability

```{r, ref.label='plot_sampling',  echo=FALSE, out.width="50%", fig.show='hold'}
```

---

### Taking into account compositional and count variability

```{r, ref.label='plot_sampling',  echo=FALSE, out.width="50%", fig.show='hold'}
```

---

### Taking into account compositional and count variability

```{r, ref.label='plot_sampling',  echo=FALSE, out.width="50%", fig.show='hold'}
```

---

### Taking into account compositional and count variability

```{r, ref.label='plot_sampling',  echo=FALSE, out.width="50%", fig.show='hold'}
```

---

### Ensembling

* Cluster-based Similarity Partitioning Algorithm (CSPA) (Strehl and Ghosh, 2002)
* Majority voting (Dudoit and Fridlyand, 2003)

```{r, echo=FALSE}
gen_label_zr = function(){
  Hs_zr = apply(Y, 1, function(x_) c_rlrnm_mixture_posterior(1, x_, PI_zr, MU_zr, SIGMA_zr, t(MASS::ginv(B2)), r = 100)) %>% 
    t() %>% as.data.table()
  kmeansruns(Hs_zr, krange = 1:10)$cluster
}
gen_label_nz = function(){
  Hs_nz = apply(Y, 1, function(x_) c_rlrnm_mixture_posterior(1, x_, PI_nz, MU_nz, SIGMA_nz, t(MASS::ginv(B2)), r = 100)) %>% 
    t() %>% as.data.table()
  kmeansruns(Hs_nz, krange = 1:10)$cluster
}
C_zr = replicate(100, gen_label_zr())
C_nz = replicate(100, gen_label_nz())
```

```{r, echo=FALSE}
Cx_zr = array(0, dim = c(nrow(Y), ncol(C_zr), 1, 1), dimnames = list(1:nrow(Y), 1:ncol(C_zr), 1, max(C_zr)))
Cx_zr[,,1,1] = C_zr
C_mv_zr = majority_voting(Cx_zr)
Cx_nz = array(0, dim = c(nrow(Y), ncol(C_nz), 1, 1), dimnames = list(1:nrow(Y), 1:ncol(C_nz), 1, max(C_nz)))
Cx_nz[,,1,1] = C_nz
C_mv_nz = majority_voting(Cx_nz)
# 
# table(C_mv_zr)
# table(C_mv_nz)
```

```{r, echo=FALSE, out.width="48%", fig.show='hold'}
ggtern() +
  geom_mask() +
  geom_point(data = data.table(Y), aes(x=other, y=ind, z=esp, col = factor(3-C_mv_zr)), alpha=0.8) +
  labs(title = ZR_LAB)
ggtern() +
  geom_mask() +
  geom_point(data = data.table(Y), aes(x=other, y=ind, z=esp, col = factor(C_mv_nz)), alpha=0.8) +
  labs(title = NZ_LAB)
```

---

#### Conclusions

* We have seen different approaches to cluster count data when only the relative relation relation between parts was of interest.
* Methods using compositional covariance (those based on the normality) have very limited applicability. They are computational demanding.
* A parsimonious approach can be constructed in such a way that the variability comming from a multinomial counting process can be incorporated to the observed compositional variability.
* To obtain a final clustering, consensus clustering technics can be applied to the different clustering obtained in the resampling.
